{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8e758-8e52-477b-b544-9600a88906ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65792050-2229-4ce5-a8c2-6d59e9b61e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID']='hive'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='#202203sdf'\n",
    "os.environ['AWS_REGION']='us-east-1'\n",
    "           \n",
    "# spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.13.1,org.apache.iceberg:iceberg-hive-runtime:0.13.1,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.15.40,software.amazon.awssdk:url-connection-client:2.15.40  --conf spark.sql.catalog.demo=org.apache.iceberg.spark.SparkCatalog    --conf spark.sql.catalog.demo.uri=thrift://localhost:9083  --conf spark.sql.catalog.demo.type=hive      --conf spark.sql.catalog.demo.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf  spark.sql.catalog.demo.warehouse=s3a://warehouse/   --conf spark.sql.catalog.demo.s3.endpoint=http://localhost:9000   --conf spark.sql.defaultCatalog=demo --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n",
    "# --conf spark.sql.catalog.spark_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO\n",
    "# --conf spark.sql.catalog.spark_catalog.s3.endpoint=http://minio:9000 \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.13.1,org.apache.iceberg:iceberg-hive-runtime:0.13.1,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.15.40,software.amazon.awssdk:url-connection-client:2.15.40 --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog --conf spark.sql.catalog.spark_catalog.type=hive --conf spark.sql.catalog.spark_catalog.cache-enabled=false  --conf spark.sql.catalog.spark_catalog.warehouse=s3a://hive/ --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083 --conf spark.sql.catalog.spark_catalog.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.sql.catalog.spark_catalog.s3a.endpoint=http://minio:9000 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "tableName = \"tbltestpython\"\n",
    "\n",
    "# Enable SQL/DML commands and Metastore tables for the current spark session.\n",
    "# We need to set the following configs\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "aws_key = 'hive'\n",
    "aws_secret = '#202203sdf'\n",
    "spark = SparkSession.builder.appName(\"quickstart_sql\").master(\"local[*]\") \\\n",
    "  .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "  .config('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000') \\\n",
    "  .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "  .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "  .config('spark.hadoop.fs.s3a.access.key', aws_key) \\\n",
    "  .config('spark.hadoop.fs.s3a.secret.key',  aws_secret) \\\n",
    "  .config(\"hive.metastore.warehouse.dir\", \"s3a://hive/\") \\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"s3a://hive/\") \\\n",
    "  .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"s3a://hive/\") \\\n",
    "  .config(\"spark.hadoop.metastore.catalog.default\", \"hive\") \\\n",
    "  .enableHiveSupport().getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da82a3b-14e0-48e2-a8d8-c017d9f580c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f43326-b87d-4d78-82b6-fe533ca45376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hive'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24fd7f25-bc03-46f7-b6e4-4c667faa5f4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalog.spark_catalog.warehouse', 's3a://hive/'),\n",
       " ('spark.files',\n",
       "  'file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.hadoop.fs.s3a.aws.credentials.provider',\n",
       "  'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.app.name', 'quickstart_sql'),\n",
       " ('spark.hadoop.fs.s3a.path.style.access', 'True'),\n",
       " ('spark.sql.catalog.spark_catalog.hadoop.fs.s3a.endpoint',\n",
       "  'http://minio:9000'),\n",
       " ('spark.app.id', 'local-1652831141105'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.jars',\n",
       "  'file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.app.startTime', '1652831140055'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://dc885543656c:39779/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,spark://dc885543656c:39779/jars/org.reactivestreams_reactive-streams-1.0.2.jar,spark://dc885543656c:39779/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,spark://dc885543656c:39779/jars/software.amazon.awssdk_bundle-2.15.40.jar,spark://dc885543656c:39779/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,spark://dc885543656c:39779/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,spark://dc885543656c:39779/jars/org.slf4j_slf4j-api-1.7.28.jar,spark://dc885543656c:39779/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,spark://dc885543656c:39779/jars/software.amazon.eventstream_eventstream-1.0.1.jar,spark://dc885543656c:39779/jars/software.amazon.awssdk_annotations-2.15.40.jar,spark://dc885543656c:39779/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar,spark://dc885543656c:39779/jars/software.amazon.awssdk_utils-2.15.40.jar,spark://dc885543656c:39779/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,spark://dc885543656c:39779/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,/home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar,/home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,/home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,/home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,/home/jovyan/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,/home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,/home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,/home/jovyan/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', 's3a://hive/'),\n",
       " ('spark.hadoop.hive.metastore.warehouse.dir', 's3a://hive/'),\n",
       " ('spark.sql.catalog.spark_catalog.cache-enabled', 'false'),\n",
       " ('spark.sql.extensions',\n",
       "  'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.sql.catalog.spark_catalog.uri', 'thrift://hive-metastore:9083'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.hadoop.fs.s3a.secret.key', '#202203sdf'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.catalog.spark_catalog.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.hadoop.fs.s3a.access.key', 'hive'),\n",
       " ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.2_2.12-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.iceberg_iceberg-hive-runtime-0.13.1.jar,file:///home/jovyan/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_bundle-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_url-connection-client-2.15.40.jar,file:///home/jovyan/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.901.jar,file:///home/jovyan/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,file:///home/jovyan/.ivy2/jars/software.amazon.eventstream_eventstream-1.0.1.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_utils-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_annotations-2.15.40.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_http-client-spi-2.15.40.jar,file:///home/jovyan/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.2.jar,file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.28.jar,file:///home/jovyan/.ivy2/jars/software.amazon.awssdk_metrics-spi-2.15.40.jar'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.driver.port', '39779'),\n",
       " ('spark.driver.host', 'dc885543656c')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169866a6-ee1d-4a5c-8a3e-04225511c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     hive|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5215d482-e217-41d7-965a-7e6ff08a4e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|     hive|       newdata|      false|\n",
      "|     hive|tbltestpython2|      false|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac18ea3-3390-4bc9-b220-c8f696e49b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"drop table if exists {tableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a6abc-6322-40e6-b155-2e6bb94b87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE TABLE if not exists {tableName}2(id LONG) USING iceberg location 's3a://hive/{tableName}2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701e6986-7015-411b-b730-f715c04df557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE TABLE if not exists {tableName}2(id LONG) USING iceberg  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29309de0-216d-4db2-bd8a-a6576f5b4a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE spark_catalog.hive.tbltestpython (\\n  `id` BIGINT)\\nUSING iceberg\\nLOCATION 's3a://hive/tbltestpython'\\nTBLPROPERTIES(\\n  'current-snapshot-id' = 'none',\\n  'format' = 'iceberg/parquet')\\n|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show create table tbltestpython \").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7026f9af-ee82-4f62-9b52-15d5f78fd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {tableName}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b2543a-238a-46c2-acfd-df9c91aded7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists newData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baaaa7a3-ca16-4dd4-a1f4-7f7dbae35795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE if not exists newData(id LONG) USING parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6398314-1a9f-48c1-9c8d-b85a7c536c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE spark_catalog.hive.tbltestpython2 (\\n  `id` BIGINT)\\nUSING iceberg\\nLOCATION 's3a://hive//tbltestpython2'\\nTBLPROPERTIES(\\n  'current-snapshot-id' = 'none',\\n  'format' = 'iceberg/parquet')\\n|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show create table {tableName}2\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59ad8f8d-c67a-4977-956d-8a75bcd7f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Creating a table ###############\n",
      "############ Reading the table ###############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "########### Upsert new data #############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  3|\n",
      "|  4|\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "########## Overwrite the table ###########\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "########### Update to the table(add 100 to every even value) ##############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|106|\n",
      "|108|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "######### Delete every even value ##############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "######## Read old data using time travel ############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Create a table\n",
    "    print(\"############# Creating a table ###############\")\n",
    "    spark.sql(\"drop table if exists %s\" % tableName)\n",
    "    spark.sql(f\"CREATE TABLE if not exists {tableName}(id LONG) USING iceberg location 's3a://hive/{tableName}' \")\n",
    "    spark.sql(\"truncate table %s\" % tableName)\n",
    "    spark.sql(\"INSERT INTO %s VALUES 0, 1, 2, 3, 4\" % tableName)\n",
    "\n",
    "    # Read the table\n",
    "    print(\"############ Reading the table ###############\")\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Upsert (merge) new data\n",
    "    print(\"########### Upsert new data #############\")\n",
    "    spark.sql(\"drop table if exists newData\")\n",
    "    spark.sql(f\"CREATE TABLE if not exists newData(id LONG) USING iceberg location 's3a://hive/{tableName}'   \")\n",
    "    spark.sql(\"truncate table newData\")\n",
    "    spark.sql(\"INSERT INTO newData VALUES 3, 4, 5, 6\")\n",
    "\n",
    "    spark.sql('''MERGE INTO {0} USING newData\n",
    "            ON {0}.id = newData.id\n",
    "            WHEN MATCHED THEN\n",
    "              UPDATE SET {0}.id = newData.id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        '''.format(tableName))\n",
    "\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Update table data\n",
    "    print(\"########## Overwrite the table ###########\")\n",
    "    spark.sql(\"INSERT OVERWRITE %s select * FROM (VALUES 5, 6, 7, 8, 9) x (id)\" % tableName)\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Update every even value by adding 100 to it\n",
    "    print(\"########### Update to the table(add 100 to every even value) ##############\")\n",
    "    spark.sql(\"UPDATE {0} SET id = (id + 100) WHERE (id % 2 == 0)\".format(tableName))\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Delete every even value\n",
    "    print(\"######### Delete every even value ##############\")\n",
    "    spark.sql(\"DELETE FROM {0} WHERE (id % 2 == 0)\".format(tableName))\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Read old version of data using time travel\n",
    "    print(\"######## Read old data using time travel ############\")\n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).table(tableName)\n",
    "    df.show()\n",
    "\n",
    "finally:\n",
    "    # cleanup\n",
    "    print(\"fin\")\n",
    "    # spark.sql(\"select * from \" + tableName ).show()\n",
    "    #spark.sql(\"DROP TABLE IF EXISTS \" + tableName)\n",
    "    #spark.sql(\"DROP TABLE IF EXISTS newData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a990934-7940-4f5e-b2f7-33d3df798cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"UPDATE {0} SET id = (id + 100) WHERE (id % 2 == 1)\".format(tableName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532eaadb-c0ab-4b50-ad29-fbc6a2aab791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|107|\n",
      "|109|\n",
      "|105|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(tableName).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45569c68-be8d-4ec6-8b80-ae6443d81396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0f37a-dc76-4c07-8042-b447cd89e376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
