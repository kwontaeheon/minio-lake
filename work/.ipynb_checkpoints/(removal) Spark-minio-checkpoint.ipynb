{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3657d55a-38d7-4a74-a7ae-8de488b13c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-285d94a1-3ee3-4188-83f1-5d3f6c036072;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.2.1/delta-core_2.12-1.2.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.2.1!delta-core_2.12.jar (1175ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/1.2.1/delta-storage-1.2.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;1.2.1!delta-storage.jar (148ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (175ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (167ms)\n",
      ":: resolution report :: resolve 6159ms :: artifacts dl 1670ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-285d94a1-3ee3-4188-83f1-5d3f6c036072\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (3348kB/8ms)\n",
      "22/05/10 07:12:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Spark context Web UI available at http://b21935a857af:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1652166741716).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.15)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "!spark-shell \\\n",
    "  --packages io.delta:delta-core_2.12:1.2.1 \\\n",
    "  --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\n",
    "  --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4def84c-cb54-4d7b-a7c9-96f5f94e87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Creating a table ###############\n",
      "############ Reading the table ###############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  1|\n",
      "|  4|\n",
      "|  3|\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "########### Upsert new data #############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "########## Overwrite the table ###########\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  7|\n",
      "|  9|\n",
      "|  5|\n",
      "|  8|\n",
      "+---+\n",
      "\n",
      "########### Update to the table(add 100 to every even value) ##############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|108|\n",
      "|  7|\n",
      "|106|\n",
      "|  9|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "######### Delete every even value ##############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  9|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "######## Read old data using time travel ############\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "|  1|\n",
      "|  4|\n",
      "|  3|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:1.2.1 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "tableName = \"tbltestpython\"\n",
    "\n",
    "# Enable SQL/DML commands and Metastore tables for the current spark session.\n",
    "# We need to set the following configs\n",
    "\n",
    "spark = SparkSession.builder.appName(\"quickstart_sql\").master(\"local[*]\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n",
    "\n",
    "# Clear any previous runs\n",
    "spark.sql(\"DROP TABLE IF EXISTS \" + tableName)\n",
    "spark.sql(\"DROP TABLE IF EXISTS newData\")\n",
    "\n",
    "try:\n",
    "    # Create a table\n",
    "    print(\"############# Creating a table ###############\")\n",
    "    spark.sql(\"CREATE TABLE %s(id LONG) USING delta\" % tableName)\n",
    "    spark.sql(\"INSERT INTO %s VALUES 0, 1, 2, 3, 4\" % tableName)\n",
    "\n",
    "    # Read the table\n",
    "    print(\"############ Reading the table ###############\")\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Upsert (merge) new data\n",
    "    print(\"########### Upsert new data #############\")\n",
    "    spark.sql(\"CREATE TABLE newData(id LONG) USING parquet\")\n",
    "    spark.sql(\"INSERT INTO newData VALUES 3, 4, 5, 6\")\n",
    "\n",
    "    spark.sql('''MERGE INTO {0} USING newData\n",
    "            ON {0}.id = newData.id\n",
    "            WHEN MATCHED THEN\n",
    "              UPDATE SET {0}.id = newData.id\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        '''.format(tableName))\n",
    "\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Update table data\n",
    "    print(\"########## Overwrite the table ###########\")\n",
    "    spark.sql(\"INSERT OVERWRITE %s select * FROM (VALUES 5, 6, 7, 8, 9) x (id)\" % tableName)\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Update every even value by adding 100 to it\n",
    "    print(\"########### Update to the table(add 100 to every even value) ##############\")\n",
    "    spark.sql(\"UPDATE {0} SET id = (id + 100) WHERE (id % 2 == 0)\".format(tableName))\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Delete every even value\n",
    "    print(\"######### Delete every even value ##############\")\n",
    "    spark.sql(\"DELETE FROM {0} WHERE (id % 2 == 0)\".format(tableName))\n",
    "    spark.sql(\"SELECT * FROM %s\" % tableName).show()\n",
    "\n",
    "    # Read old version of data using time travel\n",
    "    print(\"######## Read old data using time travel ############\")\n",
    "    df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).table(tableName)\n",
    "    df.show()\n",
    "\n",
    "finally:\n",
    "    # cleanup\n",
    "    spark.sql(\"DROP TABLE IF EXISTS \" + tableName)\n",
    "    spark.sql(\"DROP TABLE IF EXISTS newData\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dacf4a-944a-41e8-8161-be49aa081959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
